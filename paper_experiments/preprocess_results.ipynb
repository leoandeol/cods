{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c577f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from cods.od.visualization import plot_preds, create_pdf_with_plots\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f29d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"results-exp-detr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f95d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./{name}.pkl\", \"rb\") as file:\n",
    "    results = pkl.load(file)\n",
    "\n",
    "print(len(results))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"  Confidence Metrics:\")\n",
    "    print(f\"    Set Size: {value.confidence_set_sizes.mean()}\")\n",
    "    print(f\"    Risk: {value.confidence_coverages.mean()}\")\n",
    "    print(\"  Localization Metrics:\")\n",
    "    print(f\"    Set Size: {value.localization_set_sizes.mean()}\")\n",
    "    print(f\"    Risk: {value.localization_coverages.mean()}\")\n",
    "    print(\"  Classification Metrics:\")\n",
    "    print(f\"    Set Size: {value.classification_set_sizes.mean()}\")\n",
    "    print(f\"    Risk: {value.classification_coverages.mean()}\")\n",
    "    print(f\"  Global Risk: {value.global_coverage.mean()}\")\n",
    "    print(\"-\" * 50)\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the data for the DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate through the results dictionary\n",
    "for key, value in results.items():\n",
    "    data.append(\n",
    "        {\n",
    "            \"Key\": key,\n",
    "            \"Confidence Set Size (Mean)\": value.confidence_set_sizes.mean(),\n",
    "            \"Confidence Risk (Mean)\": value.confidence_coverages.mean(),\n",
    "            \"Localization Set Size (Mean)\": value.localization_set_sizes.mean(),\n",
    "            \"Localization Risk (Mean)\": value.localization_coverages.mean(),\n",
    "            \"Classification Set Size (Mean)\": value.classification_set_sizes.mean(),\n",
    "            \"Classification Risk (Mean)\": value.classification_coverages.mean(),\n",
    "            \"Global Risk (Mean)\": value.global_coverage.mean(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(df)\n",
    "metrics = [\n",
    "    \"Global Risk (Mean)\",\n",
    "    \"Confidence Set Size (Mean)\",\n",
    "    \"Confidence Risk (Mean)\",\n",
    "    \"Localization Set Size (Mean)\",\n",
    "    \"Localization Risk (Mean)\",\n",
    "    \"Classification Set Size (Mean)\",\n",
    "    \"Classification Risk (Mean)\",\n",
    "]\n",
    "\n",
    "for metric in metrics:\n",
    "    df[metric] = df[metric].apply(lambda x: float(x.item()))\n",
    "\n",
    "# df.to_csv(\"output-yolo.csv\")\n",
    "# # Plot each metric\n",
    "# fig, axes = plt.subplots(\n",
    "#     len(metrics),\n",
    "#     1,\n",
    "#     figsize=(12, 18 * len(metrics)),  # , sharex=True\n",
    "# )\n",
    "\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     axes[i].barh(df[\"Key\"], df[metric], color=\"skyblue\")\n",
    "#     axes[i].set_title(f\"Comparison of {metric} Across Keys\")\n",
    "#     axes[i].set_xlabel(metric)\n",
    "#     axes[i].set_ylabel(\"Key\")\n",
    "#     if \"Set Size\" not in metric:\n",
    "#         alphas = df[\"Key\"].tolist()[0].split(\"-\")[1]\n",
    "#         alphas = ast.literal_eval(alphas)\n",
    "#         if \"Global\" in metric:\n",
    "#             alpha = sum(alphas[1:])\n",
    "#         elif \"Confidence\" in metric:\n",
    "#             alpha = alphas[0]\n",
    "#         elif \"Localization\" in metric:\n",
    "#             alpha = alphas[1]\n",
    "#         elif \"Classification\" in metric:\n",
    "#             alpha = alphas[2]\n",
    "#         axes[i].axvline(\n",
    "#             x=alpha, color=\"red\", linestyle=\"--\", label=f\"Threshold = {alpha}\"\n",
    "#         )\n",
    "#     axes[i].legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# Proper Dataframe Construction\n",
    "# Define the lists provided by the user\n",
    "# Sort by length descending to match longer strings first (e.g., 'box_count_threshold' before 'box')\n",
    "alphas_str = [\n",
    "    \"[0.02, 0.05, 0.05]\",\n",
    "    \"[0.03, 0.1, 0.1]\",\n",
    "]  # Keep as strings for matching\n",
    "matching_functions = sorted([\"mix\", \"hausdorff\", \"lac\", \"giou\"], key=len, reverse=True)\n",
    "confidence_methods = sorted(\n",
    "    [\"box_count_threshold\", \"box_count_recall\", \"box_thresholded_distance\"],\n",
    "    key=len,\n",
    "    reverse=True,\n",
    ")\n",
    "localization_methods = sorted([\"thresholded\", \"pixelwise\", \"boxwise\"], key=len, reverse=True)\n",
    "classification_prediction_sets = sorted([\"lac\", \"aps\"], key=len, reverse=True)\n",
    "localization_prediction_sets = sorted([\"additive\", \"multiplicative\"], key=len, reverse=True)\n",
    "\n",
    "\n",
    "# Define the improved extraction function\n",
    "def extract_key_components_revised(key_string):\n",
    "    alpha_cnf = None\n",
    "    alpha_loc = None\n",
    "    alpha_cls = None\n",
    "    alpha_tot = None\n",
    "    matching_function = None\n",
    "    confidence_method = None\n",
    "    localization_method = None\n",
    "    classification_set = None\n",
    "    localization_set = None\n",
    "\n",
    "    try:\n",
    "        # 1. Extract Alpha\n",
    "        match = re.search(r\"alpha-(.*?)-\", key_string)\n",
    "        if match:\n",
    "            alpha = match.group(1)\n",
    "            alpha = eval(alpha)\n",
    "            alpha_cnf = alpha[0]\n",
    "            alpha_loc = alpha[1]\n",
    "            alpha_cls = alpha[2]\n",
    "            alpha_tot = alpha_loc + alpha_cls\n",
    "            # Remaining string starts after 'alpha-[alpha_val]-'\n",
    "            remaining_string = key_string[len(f\"alpha-{alpha}-\") :]\n",
    "        else:\n",
    "            return pd.Series([None] * 6)  # Return Nones if basic structure fails\n",
    "\n",
    "        # 2. Extract Matching Function\n",
    "        for mf in matching_functions:\n",
    "            if remaining_string.startswith(mf + \"_\"):\n",
    "                matching_function = mf\n",
    "                remaining_string = remaining_string[len(mf + \"_\") :]\n",
    "                break\n",
    "\n",
    "        # 3. Extract Confidence Method\n",
    "        for cm in confidence_methods:\n",
    "            if remaining_string.startswith(cm + \"_\"):\n",
    "                confidence_method = cm\n",
    "                remaining_string = remaining_string[len(cm + \"_\") :]\n",
    "                break\n",
    "\n",
    "        # 4. Extract Localization Method\n",
    "        for lm in localization_methods:\n",
    "            if remaining_string.startswith(lm + \"_\"):\n",
    "                localization_method = lm\n",
    "                remaining_string = remaining_string[len(lm + \"_\") :]\n",
    "                break\n",
    "\n",
    "        # 5. Extract Classification Prediction Set\n",
    "        for cps in classification_prediction_sets:\n",
    "            # Check if it's the last component or followed by '_'\n",
    "            if remaining_string.startswith(cps + \"_\"):\n",
    "                classification_set = cps\n",
    "                remaining_string = remaining_string[len(cps + \"_\") :]\n",
    "                break\n",
    "            elif remaining_string == cps:  # Handle case where it's the last component\n",
    "                classification_set = cps\n",
    "                remaining_string = \"\"\n",
    "                break\n",
    "\n",
    "        # 6. Extract Localization Prediction Set (the remainder)\n",
    "        # Check against the known list, otherwise assign the remainder\n",
    "        found_lps = False\n",
    "        for lps in localization_prediction_sets:\n",
    "            if remaining_string == lps:\n",
    "                localization_set = lps\n",
    "                found_lps = True\n",
    "                break\n",
    "        # If no exact match from the list, assign the remaining string\n",
    "        # (This handles potential unexpected values or if the list is incomplete)\n",
    "        # Update: Based on the expected structure, the last part *should* be the localization set.\n",
    "        if not found_lps and remaining_string in localization_prediction_sets:\n",
    "            localization_set = remaining_string\n",
    "\n",
    "        # If after matching classification_set, the remaining string is exactly one of the localization_sets\n",
    "        if classification_set and remaining_string in localization_prediction_sets:\n",
    "            localization_set = remaining_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing key: {key_string} - {e}\")\n",
    "        # Return Nones if any error occurs during parsing\n",
    "        return pd.Series(\n",
    "            [None] * 6,\n",
    "            index=[\n",
    "                \"Confidence Alpha\",\n",
    "                \"Localization Alpha\",\n",
    "                \"Classification Alpha\",\n",
    "                \"Global Alpha\",\n",
    "                \"Matching Function\",\n",
    "                \"Confidence Method\",\n",
    "                \"Localization Method\",\n",
    "                \"Classification Prediction Set\",\n",
    "                \"Localization Prediction Set\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return pd.Series(\n",
    "        [\n",
    "            alpha_cnf,\n",
    "            alpha_loc,\n",
    "            alpha_cls,\n",
    "            alpha_tot,\n",
    "            matching_function,\n",
    "            confidence_method,\n",
    "            localization_method,\n",
    "            classification_set,\n",
    "            localization_set,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply the revised function to the 'Key' column\n",
    "new_columns = df[\"Key\"].apply(extract_key_components_revised)\n",
    "new_columns.columns = [\n",
    "    \"Confidence Alpha\",\n",
    "    \"Localization Alpha\",\n",
    "    \"Classification Alpha\",\n",
    "    \"Global Alpha\",\n",
    "    \"Matching Function\",\n",
    "    \"Confidence Method\",\n",
    "    \"Localization Method\",\n",
    "    \"Classification Prediction Set\",\n",
    "    \"Localization Prediction Set\",\n",
    "]\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame\n",
    "df_updated = pd.concat([df.drop(\"Key\", axis=1, errors=\"ignore\"), new_columns], axis=1)\n",
    "\n",
    "\n",
    "# Display the first 5 rows with the new columns\n",
    "print(\"DataFrame with correctly decomposed 'Key' column:\")\n",
    "print(df_updated.head().to_string(index=False))\n",
    "\n",
    "# Print the column names and their data types\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df_updated.info())\n",
    "\n",
    "# Display unique values for verification\n",
    "print(\"\\nUnique values in new columns:\")\n",
    "for col in new_columns.columns:\n",
    "    # Show None separately if present\n",
    "    unique_vals = df_updated[col].unique()\n",
    "    unique_vals_list = [str(v) for v in unique_vals if pd.notna(v)]\n",
    "    if df_updated[col].isna().any():\n",
    "        unique_vals_list.append(\"None\")\n",
    "    print(f\"- {col}: {unique_vals_list}\")\n",
    "    # print(f\"- {col}: {df[col].unique()}\")\n",
    "df2 = df_updated.copy()\n",
    "df2.to_csv(f\"output-{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b2888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd9c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
