{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal Object Detection: first steps\n",
    "\n",
    "This tutorial should get you started doing **Conformal Object Detection (COD)** with the [`cods`](https://github.com/leoandeol/cods) library.\n",
    "\n",
    "For more information on the methods implemented in CODS, see the papers: \n",
    "- [And茅ol et al. 2023: Confident Object Detection via Conformal Prediction and Conformal Risk Control](https://proceedings.mlr.press/v204/andeol23a.html)\n",
    "- [Angelopoulos et al. 2022: Conformal Risk Control](https://arxiv.org/abs/2208.02814)\n",
    "- [Li et al. 2022: Towards PAC Multi-Object Detection and Tracking](https://arxiv.org/abs/2204.07482)\n",
    "- [Bates et al. 2021: Risk Controlling Prediction Sets](https://dl.acm.org/doi/abs/10.1145/3478535)\n",
    "\n",
    "\n",
    "### Get started\n",
    "1. Download the MS-COCO dataset: \n",
    "    - https://cocodataset.org/\n",
    "2. Download DETR: automatically via Pytorch hub: https://pytorch.org/hub/\n",
    "    - source: https://github.com/facebookresearch/detr\n",
    "\n",
    "### Contents\n",
    "What we will be doing:\n",
    "1. Setup inference [猡](#Setup-inferences)\n",
    "    - load predictor (DETR) pretrained on COCO\n",
    "    - Split the validation into: calibration & validation dataset\n",
    "2. Run inferences on these datasets [猡](#Setup-inferences)\n",
    "    - Save predictions to disk: faster than re-predict for every test\n",
    "3. Test Conformal Prediction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.data import MSCOCODataset\n",
    "from cods.od.models import DETRModel\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = (\n",
    "    \"0\"  # chose the GPU. If only one, then \"0\"\n",
    ")\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup inferences [](#conformal-object-detection-first-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set [COCO_PATH] to the directory to your local copy of the COCO dataset\n",
    "COCO_PATH = \"/datasets/shared_datasets/coco/\"\n",
    "\n",
    "data = MSCOCODataset(root=COCO_PATH, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/leo.andeol/.cache/torch/hub/facebookresearch_detr_main\n",
      "/home/leo.andeol/envs/cods_13/cods/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/leo.andeol/envs/cods_13/cods/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data) = 5000\n",
      "len(data_cal) = 400\n",
      "len(data_val) = 400\n"
     ]
    }
   ],
   "source": [
    "calibration_ratio = (\n",
    "    0.5  # set 0.5 to use 50% for calibration and 50% for testing\n",
    ")\n",
    "\n",
    "use_smaller_subset = True  # TODO: Temp\n",
    "\n",
    "if use_smaller_subset:\n",
    "    data_cal, data_val = data.random_split(\n",
    "        calibration_ratio, shuffled=False, n_calib_test=800\n",
    "    )\n",
    "else:\n",
    "    data_cal, data_val = data.random_split(calibration_ratio, shuffled=False)\n",
    "\n",
    "# model and weights are downloaded from https://github.com/facebookresearch/detr\n",
    "detr = DETRModel(model_name=\"detr_resnet50\", pretrained=True)\n",
    "\n",
    "print(f\"{len(data) = }\")\n",
    "print(f\"{len(data_cal) = }\")\n",
    "print(f\"{len(data_val) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inferences:\n",
    "- the first time, run inferences and save them disk\n",
    "- if predictions are saved on disk, load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from ./saved_predictions/detr_resnet50/mscoco/cal/predictions_object_detection.pkl\n",
      "Predictions already exist, loading them...\n",
      "Loading predictions from ./saved_predictions/detr_resnet50/mscoco/test/predictions_object_detection.pkl\n",
      "Predictions already exist, loading them...\n"
     ]
    }
   ],
   "source": [
    "preds_cal = detr.build_predictions(\n",
    "    data_cal,\n",
    "    dataset_name=\"mscoco\",\n",
    "    split_name=\"cal\",\n",
    "    batch_size=12,\n",
    "    collate_fn=data._collate_fn,\n",
    "    shuffle=False,\n",
    "    force_recompute=False,\n",
    ")\n",
    "preds_val = detr.build_predictions(\n",
    "    data_val,\n",
    "    dataset_name=\"mscoco\",\n",
    "    split_name=\"test\",\n",
    "    batch_size=12,\n",
    "    collate_fn=data._collate_fn,\n",
    "    shuffle=False,\n",
    "    force_recompute=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo.andeol/envs/cods_13/cods/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[WARNING:cp.py:cp:1217 - __init__ ] No multiple_testing_correction provided, assuming no correction is needed. The explicit list of alphas is expected for calibration.\n",
      "[INFO:cp.py:cp:181 - __init__ ] Defaulting to CRC backend\n",
      "[INFO:cp.py:cp:900 - __init__ ] Defaulting to CRC backend\n"
     ]
    }
   ],
   "source": [
    "from cods.od.cp import ODConformalizer\n",
    "\n",
    "conf = ODConformalizer(\n",
    "    backend=\"auto\",\n",
    "    guarantee_level=\"object\",\n",
    "    matching=\"assymetric_hausdorff\",\n",
    "    multiple_testing_correction=None,\n",
    "    confidence_method=\"nb_boxes\",\n",
    "    localization_method=\"boxwise\",\n",
    "    localization_prediction_set=\"additive\",\n",
    "    classification_method=\"lac\",\n",
    "    optimizer=\"binary_search\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:cp.py:cp:1429 - calibrate ] Calibrating Confidence Conformalizer\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.08, 0.08] -> [0.0770263671875]. Corrected Risk = 0.00: 100%|| 13/13 [02:00<00:00,  9.26s/it]\n",
      "[0.08, 0.08] -> [0.0770263671875]. Corrected Risk = 0.01: 100%|| 13/13 [01:59<00:00,  9.21s/it]\n",
      "[INFO:cp.py:cp:1439 - calibrate ] Setting Confidence Threshold of Predictions\n",
      "[INFO:cp.py:cp:1448 - calibrate ] Calibrated Confidence 位 : 0.0770\n",
      "\t and associated Confidence Threshold : 0.9229736328125\n",
      "[WARNING:cp.py:cp:1457 - calibrate ] Overwriting previous matching\n",
      "[INFO:cp.py:cp:1459 - calibrate ] Matching Predictions to True Boxes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using assymetric hausdorff distance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [00:05, 69.36it/s]\n",
      "[INFO:cp.py:cp:1471 - calibrate ] Calibrating Localization Conformalizer\n",
      "[INFO:cp.py:cp:448 - calibrate ] Using overload confidence threshold: 0.9230\n",
      "[248.29, 248.54] -> [248.4130859375]. Corrected Risk = 0.05: 100%|| 13/13 [00:18<00:00,  1.40s/it]\n",
      "[INFO:cp.py:cp:468 - calibrate ] Calibrated 位 for localization: 248.53515625\n",
      "[INFO:cp.py:cp:1481 - calibrate ] Calibrated Localization 位 : 248.53515625\n",
      "[INFO:cp.py:cp:1489 - calibrate ] Calibrating Classification Conformalizer\n",
      "[INFO:cp.py:cp:1018 - calibrate ] Using overload confidence threshold: 0.9230\n",
      "[WARNING:cp.py:cp:1023 - calibrate ] Currently considering that there is only one matching prediction to each true box for classification pruposes. To add later how to aggregate if multiple preidctions matched.\n",
      "[1.00, 1.00] -> [0.9999996801489033]. Corrected Risk = 0.05: 100%|| 40/40 [00:49<00:00,  1.23s/it]\n",
      "[INFO:cp.py:cp:1042 - calibrate ] Calibrated 位 for classification: 0.9999996802071109\n",
      "[INFO:cp.py:cp:1499 - calibrate ] Calibrated Classification 位 : 0.9999996802071109\n"
     ]
    }
   ],
   "source": [
    "# TODO(leo): we can replace this by anything, doesn't even need a guarantee (confidence)\n",
    "parameters = conf.calibrate(\n",
    "    preds_cal,\n",
    "    alpha_confidence=0.01,\n",
    "    alpha_localization=0.05,\n",
    "    alpha_classification=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:cp.py:cp:1541 - conformalize ] Conformalizing Predictions\n",
      "[INFO:cp.py:cp:1544 - conformalize ] Using provided parameters for conformalization\n",
      "[INFO:cp.py:cp:1550 - conformalize ] The parameters have been computed on another set of predictions.\n",
      "[INFO:cp.py:cp:1562 - conformalize ] Conformalizing Confidence\n",
      "[INFO:cp.py:cp:1578 - conformalize ] Conformalizing Localization\n",
      "[INFO:cp.py:cp:513 - conformalize ] Using previous 位 for localization\n",
      "[INFO:cp.py:cp:528 - conformalize ] Conformalizing Localization with 位\n",
      "[INFO:cp.py:cp:1588 - conformalize ] Conformalizing Classification\n"
     ]
    }
   ],
   "source": [
    "conformal_preds = conf.conformalize(preds_val, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:cp.py:cp:1627 - evaluate ] Evaluating Confidence Conformalizer\n",
      "[INFO:cp.py:cp:1640 - evaluate ] Evaluating Localization Conformalizer\n",
      "[INFO:cp.py:cp:1653 - evaluate ] Evaluating Classification Conformalizer\n",
      "[WARNING:cp.py:cp:1074 - evaluate ] Evaluating classification conformalizer\n",
      "100%|| 400/400 [00:00<00:00, 691.42it/s]\n",
      "[INFO:cp.py:cp:1682 - evaluate ] Evaluation Results:\n",
      "[INFO:cp.py:cp:1684 - evaluate ] \t Confidence:\n",
      "[INFO:cp.py:cp:1685 - evaluate ] \t\t Risk: 0.08\n",
      "[INFO:cp.py:cp:1686 - evaluate ] \t\t Mean Set Size: 70.07\n",
      "[INFO:cp.py:cp:1690 - evaluate ] \t Localization:\n",
      "[INFO:cp.py:cp:1691 - evaluate ] \t\t Risk: 0.05\n",
      "[INFO:cp.py:cp:1692 - evaluate ] \t\t Mean Set Size: 611.44\n",
      "[INFO:cp.py:cp:1696 - evaluate ] \t Classification:\n",
      "[INFO:cp.py:cp:1697 - evaluate ] \t\t Risk: 0.04\n",
      "[INFO:cp.py:cp:1698 - evaluate ] \t\t Mean Set Size: 38.02\n",
      "[INFO:cp.py:cp:1702 - evaluate ] \t Global:\n",
      "[INFO:cp.py:cp:1703 - evaluate ] \t\t Risk: 0.94\n"
     ]
    }
   ],
   "source": [
    "# TODO: Rewrite it so we only compute the confidence loss and not the max of three. Main loss of condiecne shoudl be just itself but in calibration use the proxy maximum loss with the others\n",
    "results_val = conf.evaluate(\n",
    "    preds_val,\n",
    "    parameters=parameters,\n",
    "    conformalized_predictions=conformal_preds,\n",
    "    include_confidence_in_global=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1757850753.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [INFO:cp.py:cp:1682 - evaluate ] Evaluation Results:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[INFO:cp.py:cp:1682 - evaluate ] Evaluation Results:\n",
    "[INFO:cp.py:cp:1684 - evaluate ] \t Confidence:\n",
    "[INFO:cp.py:cp:1685 - evaluate ] \t\t Risk: 0.08\n",
    "[INFO:cp.py:cp:1686 - evaluate ] \t\t Mean Set Size: 70.07\n",
    "[INFO:cp.py:cp:1690 - evaluate ] \t Localization:\n",
    "[INFO:cp.py:cp:1691 - evaluate ] \t\t Risk: 0.05\n",
    "[INFO:cp.py:cp:1692 - evaluate ] \t\t Mean Set Size: 422.46\n",
    "[INFO:cp.py:cp:1696 - evaluate ] \t Classification:\n",
    "[INFO:cp.py:cp:1697 - evaluate ] \t\t Risk: 0.04\n",
    "[INFO:cp.py:cp:1698 - evaluate ] \t\t Mean Set Size: 41.66\n",
    "[INFO:cp.py:cp:1702 - evaluate ] \t Global:\n",
    "[INFO:cp.py:cp:1703 - evaluate ] \t\t Risk: 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Conformal Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sebastien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.cp import ODRiskConformalizer\n",
    "\n",
    "odc = ODRiskConformalizer(\n",
    "    localization_method=\"pixelwise\",\n",
    "    # objectness_method=\"box_number\",\n",
    "    # classification_method=\"lac\",\n",
    "    # multiple_testing_correction=\"bonferroni\",\n",
    "    confidence_threshold=0.2,\n",
    ")\n",
    "odc.calibrate(preds_cal, alpha=0.1)\n",
    "conf_boxes, conf_cls = odc.conformalize(preds_val)\n",
    "metrics = odc.evaluate(preds_val, conf_boxes, conf_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.cp import ODRiskConformalizer\n",
    "\n",
    "odc = ODRiskConformalizer(\n",
    "    # localization_method=\"pixelwise\",\n",
    "    # objectness_method=\"box_number\",\n",
    "    classification_method=\"lac\",\n",
    "    # multiple_testing_correction=\"bonferroni\",\n",
    "    confidence_threshold=0.2,\n",
    ")\n",
    "odc.calibrate(preds_cal, alpha=0.1)\n",
    "conf_boxes, conf_cls = odc.conformalize(preds_val)\n",
    "metrics = odc.evaluate(preds_val, conf_boxes, conf_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.cp import ODRiskConformalizer\n",
    "\n",
    "odc = ODRiskConformalizer(\n",
    "    localization_method=\"pixelwise\",\n",
    "    # objectness_method=\"box_number\",\n",
    "    confidence_threshold=0.2,\n",
    "    classification_method=\"lac\",\n",
    "    multiple_testing_correction=\"bonferroni\",\n",
    ")\n",
    "odc.calibrate(preds_cal, alpha=0.1)\n",
    "conf_boxes, conf_cls = odc.conformalize(preds_val)\n",
    "metrics = odc.evaluate(preds_val, conf_boxes, conf_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.visualization import plot_preds\n",
    "\n",
    "plot_preds(preds_val, 1, conf_boxes, conf_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.cp import ODConformalizer\n",
    "\n",
    "odc = ODConformalizer(\n",
    "    localization_method=\"min-hausdorff-additive\",\n",
    "    objectness_method=\"box_number\",\n",
    "    classification_method=\"lac\",\n",
    "    multiple_testing_correction=\"bonferroni\",\n",
    ")\n",
    "cal_output = odc.calibrate(preds_cal, alpha=0.1)\n",
    "conf_boxes, conf_cls = odc.conformalize(preds_val)\n",
    "metrics = odc.evaluate(preds_val, conf_boxes, conf_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.cp import ODRiskConformalizer\n",
    "\n",
    "odc = ODRiskConformalizer(\n",
    "    localization_method=\"pixelwise\",\n",
    "    objectness_method=\"box_number\",\n",
    "    classification_method=\"lac\",\n",
    "    multiple_testing_correction=\"bonferroni\",\n",
    ")\n",
    "odc.calibrate(preds_cal, alpha=0.1)\n",
    "conf_boxes, conf_cls = odc.conformalize(preds_val)\n",
    "metrics = odc.evaluate(preds_val, conf_boxes, conf_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.tr import ODToleranceRegion\n",
    "\n",
    "odc = ODToleranceRegion(\n",
    "    localization_loss=\"pixelwise\",\n",
    "    confidence_loss=\"box_number\",\n",
    "    classification_loss=\"lac\",\n",
    "    multiple_testing_correction=\"bonferroni\",\n",
    "    inequality=\"bernstein\",\n",
    ")\n",
    "odc.calibrate(preds_cal, alpha=0.2, delta=0.1, bounds=[0, 1000])\n",
    "conf_boxes, conf_cls = odc.conformalize(preds_val)\n",
    "metrics = odc.evaluate(preds_val, conf_boxes, conf_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.visualization import plot_preds\n",
    "\n",
    "plot_preds(preds_val, 10, conf_boxes=conf_boxes, conf_cls=conf_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cods.od.metrics import unroll_metrics\n",
    "\n",
    "unroll_metrics(od_preds=preds_val, conf_boxes=conf_boxes, conf_cls=conf_cls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cods_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
